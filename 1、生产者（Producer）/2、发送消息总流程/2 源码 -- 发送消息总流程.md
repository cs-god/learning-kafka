

```java
/* --------------------------------- KafkaProducer --------------------------------- */
public Future<RecordMetadata> send(ProducerRecord<K, V> record) {  
    return send(record, null);  
}
public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {  
    // intercept the record, which can be potentially modified; this method does not throw exceptions  
    ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);  
    return doSend(interceptedRecord, callback);  
}
```

```java
/* --------------------------------- KafkaProducer --------------------------------- */
private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {  
    // Append callback takes care of the following:  
    //  - call interceptors and user callback on completion    
    //  - remember partition that is calculated in RecordAccumulator.append    
    AppendCallbacks<K, V> appendCallbacks = new AppendCallbacks<K, V>(callback, this.interceptors, record);  
  
    try {  

        throwIfProducerClosed();  

        // first make sure the metadata for the topic is available  
        long nowMs = time.milliseconds();  
        ClusterAndWaitTime clusterAndWaitTime;  

        try {  
            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);  
        } 
        catch (KafkaException e) {  
            if (metadata.isClosed())  
                throw new KafkaException("Producer closed while send in progress", e);  
            throw e;  
        }  
        nowMs += clusterAndWaitTime.waitedOnMetadataMs;  
        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); 

        Cluster cluster = clusterAndWaitTime.cluster;  

		// 序列化key
        byte[] serializedKey;  
        try {  
            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());  
        }
        catch (ClassCastException cce) {  
            throw new SerializationException(...);  
        }  

		// 序列化value
        byte[] serializedValue;  
        try {  
            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());  
        } 
        catch (ClassCastException cce) {  
            throw new SerializationException(...);  
        }  
  
        // Try to calculate partition, but note that after this call it can be RecordMetadata.UNKNOWN_PARTITION,  
        // which means that the RecordAccumulator would pick a partition using built-in logic (which may
        // take into account broker load, the amount of data produced to each partition, etc.).  
        int partition = partition(record, serializedKey, serializedValue, cluster);  
  
        setReadOnly(record.headers());  
        Header[] headers = record.headers().toArray();  
  
        int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(
	        apiVersions.maxUsableProduceMagic(),  
            compressionType, 
            serializedKey, 
            serializedValue, 
            headers
        );  
        // 验证record的大小不能太大, 否则抛出 RecordTooLargeException
        // - 不能超过 KafkaProducer.this.maxRequestSize
        // - 不能超过 KafkaProducer.this.totalMemorySize
        ensureValidRecordSize(serializedSize);  

        long timestamp = record.timestamp() == null ? nowMs : record.timestamp();  
  
        // A custom partitioner may take advantage on the onNewBatch callback.  
        boolean abortOnNewBatch = partitioner != null;  
  
        // Append the record to the accumulator.  Note, that the actual partition may be  
        // calculated there and can be accessed via appendCallbacks.topicPartition.     
        RecordAccumulator.RecordAppendResult result = accumulator.append(
	        record.topic(), 
	        partition, 
	        timestamp, 
	        serializedKey,  
            serializedValue, 
            headers, 
            appendCallbacks, 
            remainingWaitMs, 
            abortOnNewBatch, 
            nowMs, 
            cluster
        );  

        assert appendCallbacks.getPartition() != RecordMetadata.UNKNOWN_PARTITION;  
  
        if (result.abortForNewBatch) {  
            int prevPartition = partition;  
            onNewBatch(record.topic(), cluster, prevPartition);  
            partition = partition(record, serializedKey, serializedValue, cluster);  
            if (log.isTraceEnabled()) {  
                log.trace("Retrying append due to new batch creation for topic {} partition {}. The old partition was {}", record.topic(), partition, prevPartition);  
            }  
            result = accumulator.append(
	            record.topic(), 
	            partition, 
	            timestamp, 
	            serializedKey,  
                serializedValue, 
                headers, 
                appendCallbacks, 
                remainingWaitMs, 
                false, 
                nowMs, 
                cluster
            );  
        }  
  
        // Add the partition to the transaction (if in progress) after it has been successfully  
        // appended to the accumulator. We cannot do it before because the partition may be        
        // unknown or the initially selected partition may be changed when the batch is closed        
        // (as indicated by `abortForNewBatch`). Note that the `Sender` will refuse to dequeue        
        // batches from the accumulator until they have been added to the transaction.        
        if (transactionManager != null) {  
            transactionManager.maybeAddPartition(appendCallbacks.topicPartition());  
        }  
  
        if (result.batchIsFull || result.newBatchCreated) {  
            log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), appendCallbacks.getPartition());  
            this.sender.wakeup();  
        }  
        return result.future;  
        // handling exceptions and record the errors;  
        // for API exceptions return them in the future,        
        // for other exceptions throw directly    
    } 
    catch (ApiException e) {  
	    log.debug("Exception occurred during message send:", e);  
	    if (callback != null) {  
	        TopicPartition tp = appendCallbacks.topicPartition();  
	        RecordMetadata nullMetadata = new RecordMetadata(tp, -1, -1, RecordBatch.NO_TIMESTAMP, -1, -1);  
	        callback.onCompletion(nullMetadata, e);  
	    }  
	    this.errors.record();  
	    this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);  
	    if (transactionManager != null) {  
	        transactionManager.maybeTransitionToErrorState(e);  
	    }  
	    return new FutureFailure(e);  
	} 
	catch (InterruptedException e) {  
	    this.errors.record();  
	    this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);  
	    throw new InterruptException(e);  
	} 
	catch (KafkaException e) {  
	    this.errors.record();  
	    this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);  
	    throw e;  
	} 
	catch (Exception e) {  
	    // we notify interceptor about all exceptions, since onSend is called before anything else in this method  
	    this.interceptors.onSendError(record, appendCallbacks.topicPartition(), e);  
	    throw e;  
	}  
}
```