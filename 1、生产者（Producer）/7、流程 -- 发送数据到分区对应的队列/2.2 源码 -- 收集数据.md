
```java
/* --------------------------------- RecordAccumulator --------------------------------- */
public RecordAppendResult append(
	String topic,  
    int partition,  
    long timestamp,  
    byte[] key,  
    byte[] value,  
    Header[] headers,  
    AppendCallbacks callbacks,  
    long maxTimeToBlock,  
    boolean abortOnNewBatch,  
    long nowMs,  
    Cluster cluster
) throws InterruptedException {

	// RecordAccumulator.this.topicInfoMap:
	// 类型 ConcurrentMap<String /*topic*/, TopicInfo>
	// 默认初值 new CopyOnWriteMap<>()
    TopicInfo topicInfo = topicInfoMap.computeIfAbsent(topic, k -> new TopicInfo(logContext, k, batchSize));  
  
    // We keep track of the number of appending thread to make sure we do not miss batches in  
    // abortIncompleteBatches().    
    // RecordAccumulator.this.appendsInProgress:
    // - 默认初值 null, 在构造函数中完成初始化 new AtomicInteger(0)
    appendsInProgress.incrementAndGet();  
    
    ByteBuffer buffer = null;  
    if (headers == null) 
	    // Record.EMPTY_HEADERS: 静态变量, 初值 new Header[0] 
	    headers = Record.EMPTY_HEADERS;  
    try {  
        // Loop to retry in case we encounter partitioner's race conditions.  
        while (true) {  
            // If the message doesn't have any partition affinity, so we pick a partition based on the broker  
            // availability and performance.  Note, that here we peek current partition before we hold the            
            // deque lock, so we'll need to make sure that it's not changed while we were waiting for the 
            // deque lock.            
            final BuiltInPartitioner.StickyPartitionInfo partitionInfo;  
            final int effectivePartition;  
            if (partition == RecordMetadata.UNKNOWN_PARTITION) {  
                partitionInfo = topicInfo.builtInPartitioner.peekCurrentPartitionInfo(cluster);  
                effectivePartition = partitionInfo.partition();  
            } else {  
                partitionInfo = null;  
                effectivePartition = partition;  
            }  
  
            // Now that we know the effective partition, let the caller know.  
            setPartition(callbacks, effectivePartition);  
  
            // check if we have an in-progress batch  
            // 
            Deque<ProducerBatch> dq = topicInfo.batches.computeIfAbsent(
	            effectivePartition, k -> new ArrayDeque<>()
	        );  
            synchronized (dq) {  
                // After taking the lock, validate that the partition hasn't changed and retry.  
                if (partitionChanged(topic, topicInfo, partitionInfo, dq, nowMs, cluster))  
                    continue;  
  
                RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callbacks, dq, nowMs);  
                if (appendResult != null) {  
                    // If queue has incomplete batches we disable switch (see comments in updatePartitionInfo).  
                    boolean enableSwitch = allBatchesFull(dq);  
                    topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, appendResult.appendedBytes, cluster, enableSwitch);  
                    return appendResult;  
                }  
            }  
  
            // we don't have an in-progress record batch try to allocate a new batch  
            if (abortOnNewBatch) {  
                // Return a result that will cause another call to append.  
                return new RecordAppendResult(null, false, false, true, 0);  
            }  
  
            if (buffer == null) {  
                byte maxUsableMagic = apiVersions.maxUsableProduceMagic();  
                int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));  
                log.trace("Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms", size, topic, partition, maxTimeToBlock);  
                // This call may block if we exhausted buffer space.  
                buffer = free.allocate(size, maxTimeToBlock);  
                // Update the current time in case the buffer allocation blocked above.  
                // NOTE: getting time may be expensive, so calling it under a lock                // should be avoided.                nowMs = time.milliseconds();  
            }  
  
            synchronized (dq) {  
                // After taking the lock, validate that the partition hasn't changed and retry.  
                if (partitionChanged(topic, topicInfo, partitionInfo, dq, nowMs, cluster))  
                    continue;  
  
                RecordAppendResult appendResult = appendNewBatch(topic, effectivePartition, dq, timestamp, key, value, headers, callbacks, buffer, nowMs);  
                // Set buffer to null, so that deallocate doesn't return it back to free pool, since it's used in the batch.  
                if (appendResult.newBatchCreated)  
                    buffer = null;  
                // If queue has incomplete batches we disable switch (see comments in updatePartitionInfo).  
                boolean enableSwitch = allBatchesFull(dq);  
                topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, appendResult.appendedBytes, cluster, enableSwitch);  
                return appendResult;  
            }  
        }  
    } 
    finally {  
        free.deallocate(buffer);  
        appendsInProgress.decrementAndGet();  
    }  
}
```


# 检查分区是否已经改变 -- partitionChanged()
```java
private boolean partitionChanged(
	String topic,  
    TopicInfo topicInfo,  
    BuiltInPartitioner.StickyPartitionInfo partitionInfo,  
    Deque<ProducerBatch> deque, long nowMs,  
    Cluster cluster) {  

    if (topicInfo.builtInPartitioner.isPartitionChanged(partitionInfo)) {  
        log.trace("Partition {} for topic {} switched by a concurrent append, retrying",  
                partitionInfo.partition(), topic);  
        return true;    }  
  
    // We might have disabled partition switch if the queue had incomplete batches.  
    // Check if all batches are full now and switch .    if (allBatchesFull(deque)) {  
        topicInfo.builtInPartitioner.updatePartitionInfo(partitionInfo, 0, cluster, true);  
        if (topicInfo.builtInPartitioner.isPartitionChanged(partitionInfo)) {  
            log.trace("Completed previously disabled switch for topic {} partition {}, retrying",  
                    topic, partitionInfo.partition());  
            return true;        }  
    }  
  
    return false;  
}
```

# tryAppend

```java
private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Header[] headers,  
                                     Callback callback, Deque<ProducerBatch> deque, long nowMs) {  
    if (closed)  
        throw new KafkaException("Producer closed while send in progress");  

    ProducerBatch last = deque.peekLast();  
    if (last != null) {  
        int initialBytes = last.estimatedSizeInBytes();  
        FutureRecordMetadata future = last.tryAppend(timestamp, key, value, headers, callback, nowMs);  
        if (future == null) {  
            last.closeForRecordAppends();  
        } else {  
            int appendedBytes = last.estimatedSizeInBytes() - initialBytes;  
            return new RecordAppendResult(
	            future, 
	            deque.size() > 1 || last.isFull(), 
	            false, 
	            false, 
	            appendedBytes
	        );  
        }  
    }  
    return null;  
}
```