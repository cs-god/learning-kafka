
```java
// ------------------- 1 ------------------- 
public KafkaProducer(Properties properties) {  
    this(properties, null, null);  // 到2.1
}
// ------------------- 2.1 -------------------
public KafkaProducer(Properties properties, Serializer<K> keySerializer, Serializer<V> valueSerializer) { 
	this(Utils.propsToMap(properties), keySerializer, valueSerializer);  // 到 3
}
// ------------------- 2.2 -------------------
public KafkaProducer(final Map<String, Object> configs) {  
    this(configs, null, null);  // 到 3
}
// ------------------- 3 -------------------
public KafkaProducer(Map<String, Object> configs, Serializer<K> keySerializer, Serializer<V> valueSerializer) { 
    this(
	    new ProducerConfig(ProducerConfig.appendSerializerToConfig(configs, keySerializer, valueSerializer)),  
        keySerializer, 
        valueSerializer, 
        null,   // ProducerMetadata
        null,
        null, 
        Time.SYSTEM
    );  
}
```

如何构建
```java
static Map<String, Object> appendSerializerToConfig(Map<String, Object> configs,  
        Serializer<?> keySerializer,  
        Serializer<?> valueSerializer) {  
    // validate serializer configuration, if the passed serializer instance is null, the user must explicitly set a valid serializer configuration value  
    Map<String, Object> newConfigs = new HashMap<>(configs);  
    if (keySerializer != null)  
        newConfigs.put(KEY_SERIALIZER_CLASS_CONFIG, keySerializer.getClass());  
    else if (newConfigs.get(KEY_SERIALIZER_CLASS_CONFIG) == null)  
        throw new ConfigException(KEY_SERIALIZER_CLASS_CONFIG, null, "must be non-null.");  
    if (valueSerializer != null)  
        newConfigs.put(VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer.getClass());  
    else if (newConfigs.get(VALUE_SERIALIZER_CLASS_CONFIG) == null)  
        throw new ConfigException(VALUE_SERIALIZER_CLASS_CONFIG, null, "must be non-null.");  
    return newConfigs;  
}
```

# 具体的创建过程
```java
KafkaProducer(ProducerConfig config,  
              Serializer<K> keySerializer,  
              Serializer<V> valueSerializer,  
              ProducerMetadata metadata,  
              KafkaClient kafkaClient,  
              ProducerInterceptors<K, V> interceptors,  
              Time time) {  
    try {  

        this.producerConfig = config;  

		... // time

		// 
		String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG); // "transactional.id"
        this.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);                 // "client.id"
		LogContext logContext;  
		if (transactionalId == null)  
		    logContext = new LogContext(String.format("[Producer clientId=%s] ", clientId));  
		else  
		    logContext = new LogContext(String.format("[Producer clientId=%s, transactionalId=%s] ", clientId, transactionalId));  
		log = logContext.logger(KafkaProducer.class);


        Map<String, String> metricTags = Collections.singletonMap("client-id", clientId);  
        MetricConfig metricConfig = new MetricConfig()
        .samples(
	        // METRICS_NUM_SAMPLES_CONFIG = "metrics.num.samples"
	        config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG)
	    )  
        .timeWindow(
            config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)  
            .recordLevel(
                Sensor.RecordingLevel.forName(config.getString(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG))
	        )  
        .tags(metricTags);  

        List<MetricsReporter> reporters = CommonClientConfigs.metricsReporters(clientId, config);  
        MetricsContext metricsContext = new KafkaMetricsContext(JMX_PREFIX,  
                config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));  
        this.metrics = new Metrics(metricConfig, reporters, time, metricsContext);  
        this.producerMetrics = new KafkaProducerMetrics(metrics);  

        this.partitioner = config.getConfiguredInstance(  
            ProducerConfig.PARTITIONER_CLASS_CONFIG,                             // "partitioner.class"
            Partitioner.class,
            Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)  // "client.id"
        );  
        warnIfPartitionerDeprecated();  
        // PARTITIONER_IGNORE_KEYS_CONFIG = "partitioner.ignore.keys"
        this.partitionerIgnoreKeys = config.getBoolean(ProducerConfig.PARTITIONER_IGNORE_KEYS_CONFIG); 

		// ----------------------------------------------------------------------------
		// ------------------- 初始化发送流程中的重要组件 key序列化器 -------------------
		// ----------------------------------------------------------------------------
        if (keySerializer == null) {  
            this.keySerializer = config.getConfiguredInstance(
	            ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,  // "key.serializer"
                Serializer.class
            );  
            this.keySerializer.configure(
	            config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), 
	            true
	        );  
        } 
        else {  
            config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);  // "key.serializer"
            this.keySerializer = keySerializer;  
        }  

		// ----------------------------------------------------------------------------
		// ------------------- 初始化发送流程中的重要组件 value序列化器 -----------------
		// ----------------------------------------------------------------------------
        if (valueSerializer == null) {  
            this.valueSerializer = config.getConfiguredInstance(
	            ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,  
                Serializer.class
            );  
			this.valueSerializer.configure(
				config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), 
				false
			);  
        } 
        else {  
            config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);  
            this.valueSerializer = valueSerializer;  
        }  

		// ----------------------------------------------------------------------------
		// ------------------- 初始化发送流程中的重要组件 拦截器 ------------------------
		// ----------------------------------------------------------------------------
        List<ProducerInterceptor<K, V>> interceptorList = (List) config.getConfiguredInstances(  
                ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,                              // "interceptor.classes"
                ProducerInterceptor.class,  
                Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)     // "client.id"
        );  
        if (interceptors != null)  
            this.interceptors = interceptors;  
        else            
	        this.interceptors = new ProducerInterceptors<>(interceptorList);  

        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(
	        keySerializer,  
            valueSerializer, 
            interceptorList, 
            reporters
        );  

        this.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);  
        this.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);  
        this.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));  
        this.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);  
        int deliveryTimeoutMs = configureDeliveryTimeout(config, log);  
  
        this.apiVersions = new ApiVersions();  

        this.transactionManager = configureTransactionState(config, logContext);  
        // There is no need to do work required for adaptive partitioning, if we use a custom partitioner.  
        boolean enableAdaptivePartitioning = partitioner == null &&  
            config.getBoolean(ProducerConfig.PARTITIONER_ADPATIVE_PARTITIONING_ENABLE_CONFIG);  
        RecordAccumulator.PartitionerConfig partitionerConfig = new RecordAccumulator.PartitionerConfig(  
            enableAdaptivePartitioning,  
            config.getLong(ProducerConfig.PARTITIONER_AVAILABILITY_TIMEOUT_MS_CONFIG)  
        );  

		// RETRY_BACKOFF_MS_CONFIG = "retry.backoff.ms" 
        long retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);  


        // As per Kafka producer configuration documentation batch.size may be set to 0 to explicitly disable  
        // batching which in practice actually means using a batch size of 1.        
        int batchSize = Math.max(1, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG));  
        this.accumulator = new RecordAccumulator(logContext,  
            batchSize,  
            this.compressionType,  
            lingerMs(config),  
            retryBackoffMs,  
            deliveryTimeoutMs,  
            partitionerConfig,  
            metrics,  
            PRODUCER_METRIC_GROUP_NAME,  
            time,  
            apiVersions,  
            transactionManager,  
            new BufferPool(this.totalMemorySize, batchSize, metrics, time, PRODUCER_METRIC_GROUP_NAME)
        );  
  
        List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(  
            config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),   // "bootstrap.servers"
            config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)  // "client.dns.lookup"
        );

		// ----------------------------------------------------------------------------
		// ------------------- 初始化发送流程中的重要组件 metadata ---------------------
		// ----------------------------------------------------------------------------
        if (metadata != null) {  
            this.metadata = metadata;  
        } else {  
            this.metadata = new ProducerMetadata(
	            retryBackoffMs,                                           // "retry.backoff.ms" 对应值
                config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),   // "metadata.max.age.ms"
                config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),  // "metadata.max.idle.ms"
                logContext, 
                clusterResourceListeners,  
                Time.SYSTEM
            );  
            this.metadata.bootstrap(addresses);  
        }  

        this.errors = this.metrics.sensor("errors");  

        this.sender = newSender(logContext, kafkaClient, this.metadata);  

        String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;  
        this.ioThread = new KafkaThread(ioThreadName, this.sender, true);  
        this.ioThread.start();  
        config.logUnused();  
        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());  
        log.debug("Kafka producer started");  
    } catch (Throwable t) {  
        // call close methods if internal objects are already constructed this is to prevent resource leak. see KAFKA-2121  
        close(Duration.ofMillis(0), true);  
        // now propagate the exception  
        throw new KafkaException("Failed to construct kafka producer", t);  
    }  
}
```